---
layout:     post
title:      支持向量机初探
subtitle:   
date:       2018-01-08
author:     HL
header-img: img/svm-first.jpg
catalog: true
tags:
    - machine learning
    - svm
---

# 支持向量机

##  支持向量机最简单的理解：

> 支持向量机可以用于对数据进行分类也可以用于回归。其本质是要在两类数据特征向量之间寻找一个分隔超平面，用于将两类样本分离。其主要特征在于，要寻找的超平面是可以将两类数据特征向量到超平面的距离最远；也就是不仅仅要找到一条超平面用于分隔两类数据，而且还要使得边界距离最大化。

## 在支持向量机中还有两个概念：


1. 函数距离：也就是样本点到超平面的距离 w*x+b
2. 几何距离：也就是函数距离除以w的模.

因为函数距离可以因为w和b的任意伸缩，而变的很大，因此加上约束之后，可以将函数距离之和最大化目标转为求几何距离最大化。在后面求解与推导支持向量机的过程中，将引入拉格朗日算子，将约束条件转化为目标函数的二次优化问题。在经过一系列数学推导之后，我们的目标在于求解使得超平面的margin(与支持向量之间：支持向量指的是到分隔面几何距离为1的那些向量点)最大化的目标参数w* 与b* ，其中w*，b*只与输入样本值以及拉格朗日参数alpha相关;

* w* = $\sum_i^N{\alpha_iy_ix_i}$
* b* = $\sum_i^N{\alpha_iy_i}$

因而最终只需求解出最优的alpha即可求得超平面。



## 支持向量机在数据线性不可分时可以引入kernel methods

之所以引入核方法：

* 一来是为了解决数据线性不可分问题，需要将数据映射到高维空间
* 二来是为了各方面计算，因为将输入数据映射到高维空间的计算量是很大的，而实际上将数据映射到高维空间的后也只需要计算其内积，因此如果可以直接根据输入数据计算其在高维空间的内积必然最好。kernel methods 就是这么做的！

常见的核方法有：

* linear kernel  
* rbf kernel
* polynomial kernel          $K(x,z) = (x*z+1)^p$



## 再说明一下支持向量机采用的损失函数hinge loss

$$loss = \sum_i^N{max(0,1-y_i(wx_i+b))}+\lambda||w||$$

从hinge loss 的表达式也可以很容易看出：

* 对于误分类问题，表达式$1-y_i(w_*x_i+b_*)$ 的值相当于在原始的函数距离上加1，也就是加大了误分类样本的惩罚
* 对于到分隔面的距离等于1的，并且正确分类的，loss为0
* 对于到分隔面距离小于1的也给予惩罚
* 对于到分隔面距离大于1，并且正确分类的，$1-y_i(w_*x_i+b_*)$ 小于0，SVM将这样的样本loss设为0，也就是不惩罚。
* 因此，可以看出只有决策面内以及误分类的样本起到作用。
* 最后：右边表达式最后一项是正则化项。

## SVM的优缺点

### 优点

 * 对于清晰可分的数据而言SVM works pretty well
 * 对于高维空间数据而言，SVM十分有效
 * 当样本数量大小低于样本空间维度时，SVM很有效
 * SVM只使用了训练点的一部分子集也就是支持向量，因此节省内存

### 缺点

 * 当我们的数据集很大的时候SVM表现的不是很好，因为需要很长的运行时间
 * 当数据集有很多噪声的时候，比如目标分类有很多重叠，SVM表现的不是很好
 * SVM并不直接提供概率估计，如果需要的话需要使用代价昂贵的five-fold 交叉验证。