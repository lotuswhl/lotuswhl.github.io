---
layout:     post
title:      循环神经网络概览系列之 - part 2
subtitle:   RNN overview 系列之(背景)
date:       2018-04-04
author:     HL
header-img: img/rnn-lstm.png
catalog: true
tags:
    - deep learning
    - rnn
    - lstm
---

# 背景
这一章节给出相应的notation以及简单回顾下neural networks的基础
## 序列(Sequence)
为了方便，我们使用$X^{(t)}$上标结合括号表示时间t时的输入元素，相应的$y^{(t)}$表示时间t对应的输出元素，这里的t是通常意义的时间步骤time step，而不是严格的时间。对于序列模型而言，我们的训练数据通常由$(\{X^{(1)},X^{(2)},...X^{(t)}\},\{y^{(1)},y^{(2)},...y^{(t)}\})$给出，其中每一个$X^{(t)}$都表示一个输入向量vector，y也是如此。并且没有给出下标以免混淆。使用小括号的目的在于，避免混淆时间特性和节点的标号。**与一般的神经网络的单变量输入，单变量输出不同的是RNN的输入与输出样本都是一组序列。**  另一方面来说，RNN不仅仅可以处理时间序列数据，对于其他的非时间序列问题同样适用，虽然这里一直强调时间序列，数据的时间特性，但对于其他的序列数据，方法同样适用。对于RNN而言，其输入可能是视频的某一帧，或者是音频的一个个片段，因此，实际上t所表达的并不是真正与时间相对应，而是表达一种ordinal(顺序的)的特性。  
一个通俗的例子，比如在自然语言处理问题中，对于语句:"I like to be here." 我们的输入序列可能是"I","like","to"...也就是以单词的形式作为输入，并没有对应每个duration(时间),而是强调一种的顺序特性。

## 神经网络基础
谈及神经网络，似乎离不开神经科学。对于神经网络，通常引用神经元的激活来进行解释。简单的而说，神经网络的基本元素在于其节点或者说神经单元，伴随着激活函数。一个神经网络通常由一组神经元构成，神经元之间通过一组有向的边进行连接类比生物神经网络的神经连接。与每一个神经单元j或者节点相关联的有一个激活函数$l_j()$。然后我们用$w_{jj_{'}}$表示从神经元$j_{'}$到神经元j之间的连接权重。对于神经元的j的值$v_{j}$通常由一下公式计算得到：
$$v_{j}=l_{j}(\sum_{j_{'}}{w_{jj_{'}}v_{j_{'}}})$$
为了方便，我们将上面的求和项也就是激活函数的输入标记为$a_j$ ;  
<br/>
我们通常使用的激活函数主要有simoid:
$$\delta(x)=\frac{e^{-x}}{1+e^{-x}}$$
这是早期神经网络使用最多的激活函数;之后更多的选择是tanh函数，在普通的DNN以及RNN中都有运用。
$$tanh(x)=\frac{1-e^{-x}}{1+e^{-x}}$$
另外一个被广泛运用于深度学习的各种模型中的激活函数是$relu(x)=max(0,x)$; 对于多分类的问题，我们最常用的一个激活函数，也可以理解为将节点的输出结果转化为概率分布的softmax函数：
$$softmax(x)=\frac{e^{x^i}}{\sum_j{e^j}}$$

## 前馈神经网络和反向传播
前馈神经网络实际上是一类受限网络，也就是他的节点不能存在有环图。前馈网络的特点就是可以根据前一层节点值依次往后计算后面每一层节点值。通常，将输入值替换到网络的输出层，然后依次计算后续网络层的节点值。对于分类或者回归问题，通常我们需要根据最后一层节点的输出值计算我们预期输出y,然后按照事先预定好的损失函数$L(y,\hat{y})$为每一个输出节点计算差值，然后再通过反向传播算法将错误逐层反馈给网络参数。实际上，除了反向传播算法，早期有过对于遗传算法的尝试，也取得了不错的效果，但是现在最新的深度学习算法基本都是使用反向传播算法完成梯度回传的。  
<br/>
如何反向传播梯度呢，首先我们会计算损失函数对每个预测节点或者输出节点的梯度$\frac{\partial{L(y,\hat{y})}}{\hat{y}}$.然后计算每一个节点的梯度值,比如对节点k:
$$\delta{_k}=\frac{\partial{L(y_k,\hat{y_k})}}{\hat{y_k}}l_k^{'}(a_k)$$
$l_k^{'}(a_k)$表示的是节点k的激活函数的导数，然后将相应的梯度往前传递，其前一层节点的梯度就可以这样计算：  
$$\delta{_j}=l_j^{'}(a_j)\sum_k{w_{kj}}{\delta{_k}}$$
按照上面的公式逐层往前推进。其中$\delta{_j}$表示的就是损失函数对节点j的值的梯度$\partial{L}/\partial{a_j}$。  
**反向传播的核心目的在于更新网络的参数，根据上面计算的损失对节点值的梯度我们可以得到损失函数对每一个参数的梯度：**  
$$\frac{\partial{L}}{\partial{w_ji}}=\delta{_j}v_i$$

