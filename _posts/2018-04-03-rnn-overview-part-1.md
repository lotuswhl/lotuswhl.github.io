---
layout:     post
title:      循环神经网络概览系列之 - part 1
subtitle:   RNN overview 系列之(概要)与(介绍)
date:       2018-04-03
author:     HL
header-img: img/rnn-lstm.png
catalog: true
tags:
    - deep learning
    - rnn
    - lstm
---

# 概要
<p>
深度学习目前已经取得了太多的辉煌成就,不仅仅在计算机视觉领域,自然语言处理领域,在推荐系统,语音识别等领域也有着不菲的贡献.为了更好的理解人工智能,或者为了让计算机更好的为人类服务,目前机器学习和深度学习在图像识别,对象检测等视觉领域,在文本分类,情感分析等自然语言处理领域取得了突破性的进展.
</p> 
<p>
然而我们日常生活中往往需要更多的面对序列数据,比如Image Caption,Speech Synthesis,以及音乐生成(music generation)等等都需要可以输出序列的模型[one to many rnn model].而在其他的一些领域,比如时间序列预测,视频分析和音乐信息检索等,则需要可以接受序列数据作为输入的模型[many to one rnn model].此外,对于机器翻译,机器人控制,可能就需要同时具备可以接受序列数据和输出序列数据的模型[many to many].
</p>
<p>
循环神经网络(RNN)是一类利用循环节点来捕获序列数据的动态特性的连接模型.不像前馈神经网络,RNN理论上可以捕获任意长度上下文窗口的信息. 尽管传统上来说RNN难以训练,并且经常包含上百万的参数,但是由于最新的研究进展,在优化算法,网络结构以及并行计算等方面取得了巨大的进展,使得RNN得以成功的运用于大规模的序列学习任务. 近些年,诸如LSTM(Long Short Term Memory),Bidirectional RNN等网络模型在诸多机器学习任务如机器翻译,Image Caption和手写识别等任务上取得了极大的性能突破.
</p>

# 介绍
<p>
近几年深度学习在有监督学习和无监督学习任务都取得了瞩目成就.这些方法尤其适用于机器感知任务,因为他们底层的特征往往是难以独立解释的,而神经网络模型可以学习到层次化的信息.这不同于传统的方法,他们往往适用于使用手工特征工程的任务.随着现在在存储上取得进展,我们可以拥有更多的数据集,并且基于并行计算对计算机计算能力大幅提升,浅层的线性模型往往倾向于under-fit,因此深度模型可以更好的发挥计算机的计算能力,并可以利用更多的数据资源构建出更好的模型.深度卷积神经网络通过利用视觉信息的局部相关性在诸多重要的应用上取得了记录性的成果.
</p>
<p>
尽管深度神经网络取得了诸多成就,但是他们都基于对样本的独立性假设.也就是他们都是假设输入的样本之间是独立的.(可以很容易看出,对于神经网络的输入,通常每次都是单个样本作为输入,计算输出,与之前的样本或者之后的样本无关.注意:即便是使用batch或者mini-batch进行训练,实际上在模型预测时也是对单个样本做预测).然而这对于视频的帧信息,音乐的判断或者文本语句的word显然是不成立的(独立性假设不成立).此外,对于前馈神经网络模型通常都是接受固定长度的输入向量,输出固定长度的值.因此这对于序列数据或时间数据是很不方便的.而循环神经网路可以利用其包含的隐层状态,理论上可以捕获任意长度的上下文信息,它不需要独立性假设,因此可以很好的应用于时间/序列数据.循环神经网络其输入是以序列元素为单位,也就是一次输入一个序列中的元素,并且由于隐层状态的存在,他可以选择性的通过一些元素,不通过另一些元素,从而很好的解决了输入序列元素之间依赖关系.
</p>

## why model sequentiality explicitly ?
显然支持向量机(SVM),逻辑斯蒂回归(LR)以及前馈神经网络基于独立性假设并且没有捕获时间特征也获得了不错的效果. 可以论证的来说,独立性假设在很多的机器学习任务上取得了不错的进展. 而进一步来说,很多模型也隐式的捕获了时间特性,比如通过将当前的输入与其前后节点拼接到一起作为输入,构建了一个滑动窗口上下文.  **然而尽管如此,即便独立性假设带来了不错的效果,他却抛弃了长期依赖(long term dependency).**
## 为什么不使用马尔科夫模型？
是的，马尔科夫模型尤其是其衍生品HMM（隐马尔科夫模型）具备着相当不错的特性，他主要是通过hidden state来model序列信息。**关于HMM的细则希望以后有机会可以仔细谈谈。** HMM对上下文的捕捉能力受制于其hidden state的窗口的大小，因为首先隐层状态数量为|S|的情况下，HMM使用动态规划进行inference复杂度就为$|S|^2$了。另一方面来说马尔科夫模型的当前状态只能依赖于前一个状态;当然，可以引入sliding window。比如将窗口中的隐层状态求取cross product来映射为单个向量。**具体的应用还需要进一步的探索与思考。**    
<br/>
基于对HMM的一点点理解，我们来思考RNN是如何解决这个问题的(Long Term Dependency)。RNN是一种特殊的连接模型，实际上RNN的循环模块除了有当前的输入还有上一步的隐层输出作为这一步的输入，而理论上来说，这一步的隐层状态可以捕获前面任意长度的隐层状态信息。事实上可以这样思考，对于一个具有N个节点的layer而言，即便是其节点取值为binary，也至少可以表示$2^{N}$中distinct状态。如果节点的取值可以是实值，那么将会是很大的指数级的状态表达能力。在训练和推测的时候其计算的复杂度也达到了二次级别。

## Is RNN too expressive
RNNs使用非线性激活函数构成一族强大的模型，理论上可以w按成任意的计算。一个典型的应用就是使用固定长度的RNN伴随sigmoid函数构建神经图灵机。RNN伴随非线性激活函数有着很强的表达能力,也就几乎可以完成任意的program。接下来简单的将C语言与RNN在这里进行对比，因为有人提出C语言也可以完成任意的program，但是C语言的设计之初并没有考虑machine learning的问题。也就是说，我们很难让C语言自己完成以机器学习形式的思考来实现program。究其原因，主要在于首先来说我们没有一种很简单的方法来探索C语言的参数空间，另一方面，我们也无法对C语言的参数基于一个预期设定的可微的损失函数的求导。而这确实也是我们最常用的优化方法。**再就过拟合问题来说：对于一个问题，或者program，使用C语言来实现我们有着无数的方法、参数可以完成这个目标。因此，在训练的时候，无可避免的会出现过拟合的问题。**  
那么RNN又是如何解决这些问题的呢，首先，对于给定长度的RNN，整个网络结构是可微的。RNN的设计之初就考虑了gradient-based 训练方法，因此其损失函数对其参数都是可导的。再从另外一方面来说：C程序并没有可用的正则化等防止过拟合的手段。


