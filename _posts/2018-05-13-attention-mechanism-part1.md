---
layout:     post
title:      Attention Mechanism 初探
subtitle:   关于意图机制的探索
date:       2018-05-13
author:     HL
header-img: img/attention-mechanism-1.jpg
catalog: true
tags:
    - deep learning
    - rnn
    - lstm
    - attention
---
# 意图机制的起源
当我们人在分析某个问题的时候,我们通常会阅读问题的背景知识,然后根据具体的问题去寻找蛛丝马迹.也就是我们带着目的或者意图去寻找可供利用的知识.那么在深度学习及其火热的应用领域自然语言处理和计算机视觉问题上,我们又是如何利用意图机制去解决问题的呢.  
实际上在很早的时候就已经有研究学者将意图机制应用于各种神经网络模型中,特别是在视觉问题上,因为意图本身就与人的视觉观察聚焦点有着很大关联.近几年,意图机制被广泛的运用于RNN模型用于解决自然语言处理问题,而后续又渐渐的应用于视觉问题上,比如图像的分类或者生成图像的描述等等.
# 意图机制用于解决什么样的问题
一个机制的存在或者一个方法的存在必然是用于解决某类或者某些类问题的.一个非常典型的应用就是Neural Machine Translation(NMT).  
在早期的机器翻译系统中,通常加入了大量的人工特征工程,也就是加入了很多的静态统计信息.这样的系统泛化能力往往比较差,也就是对于新的数据的适应能力比较差.而神经机器翻译NMT却有着更好的泛化能力.一个典型的NMT通常是由RNN或者LSTM等循环神经网络将一条语句进行编码为一个输出向量和一个解码器(通常也是RNN)构成.基于对编码器可以编码整条语句信息的假设,我们先将语句转为embedding然后在从这个embedding中提取信息,进行解码,解码的过程也就是翻译的过程.典型示图如下:
![NMT sample](https://raw.githubusercontent.com/lotuswhl/lotuswhl.github.io/master/img/attention/NMT-Sample.png)
在上面这张图中我们可以看到,将三个German单词的最后的隐层输出状态h3作为后面翻译或者解码器的输入.这样的实现其实很简洁而且很方便.后面的解码器会根据h3依次输出english word,直到输出一个语句结束标志.
### RNN模型语句输出向量的意义
上面的那张图中的h3,在理论上可以捕获整条语句的信息,而在实践当中我们也发现对于不同的语句输出的embedding,其距离相近着,语句或者短语的意义也很接近,这表示我们的输出向量缺失很好的捕获了语句的信息.  
一个很常见的做法就是将语句的embedding通过PCA或者t-sne等降维方法,降到二维空间上,便可以很好的观察不同的语句之间的距离并依此分析他们的语义关联.
### 存在的问题
单纯的使用这种模型有什么样的问题呢?实际上,在理论上来说如果使用LSTM,可以很好的解决长期依赖的问题,但是在实践当中却没有如此.   
考虑我们要翻译一个有着30个单词的语句,那么对于english和french来说English的翻译第一个单词可能与French的第一个单词有着很大的联系,如果是这样的话,这也就意味着我么需要在embedding中嵌入这样的信息,以便decoder可以找到.然而,在实践当中的效果却不是很好;之后研究者考虑将输出向量的语句输入单词逆序,也就是在生成embedding时反向输入单词,这样某种程度上可以拉近decoder输出单词与输入语句单词的对应距离.实践得到的效果更好.    
这在对于语言单词序列相同的英文和法文来说确实可以,当然中文与英文也有着一定的顺序关系,这算是一种hack手法,但是对于其他的语言比如日语,其顺序与英语往往相反,那么这种hack手法只会使得结果更糟.
# 引入意图机制
那么如何利用意图机制来解决上面的问题呢.一个较为粗浅的解决方法,就是我们不仅仅考虑encoder最后隐层向量的输出,而考虑全部中间隐层向量的值,在每次decoder输出一个词时都要查看输入序列的所有向量.
![attention sample](https://raw.githubusercontent.com/lotuswhl/lotuswhl.github.io/master/img/attention/attention-sample.png)  
上面的这张图给出的是一个使用的bi-directional RNN的encoder,这里可以不用管这个双向的RNN,毕竟我们的目的在于隐层的向量输出.可以到看:在预测$y_t$的时候我们根据St来进行预测,而St在到当前time step为止的所有的隐层状态的输出的weighted sum.并且这里的$a_{t*}$的和为1,也就是保证了当前输出是参考之前输入的一个分布.那么,假设$a_{4,3}$最大,那么说明在产生第4个输出单词时,更多的依赖于输入的第3个单词.
### 好处
那么这样做的好处是什么呢?首先,使用这种方式,decoder可以参考的信息更多了!可以看到之前的所有的隐层状态的信息,并从中选择需要的关注点然后产生正确的输出;其次,这种模型可以方便于观察模型的预测过程,也就是可以根据a的值(权重值)来查看网络或者模型在预测下一个单词时主要参考的目标是输入中的哪个或者哪些单词.
### 局限
这种模型的计算代价是很高的,主要来自于对输入输出单词之间的映射权重的计算.因为需要计算每一个输入单词与输出单词之间的一一对应系数;假设一条语句有30个单词,输出也是30个单词,那么至少需要计算30*30次,而如果是字符级别的连续输入与输出,其计算代价将会是二次的,甚至难以处理.
### 折中
即便意图机制的这种实现有着较高的计算复杂度,但他带来的惊喜却更多;因此并不妨碍他变得流行;当然,可能需要一些更好的实现手段以及更好的模型设计来使得他可以更好的发挥他的潜能.

# 除了机器翻译之外
意图还可以应用于比如图像描述,也就是给你一张图片,让你输出一条(或多条)描述这张图的语句.通常的实现可能是,首先由CNN模型将图像输出为一个固定长度的向量,然后作为RNN的输入,再进行描述语句的依次输出. 其中比较精彩的部分在于,我们可以观察状态权重系数a*,然后可以发现,对于每个单词的预测,其更关注于图像的什么位置.
![image desciption with rnn attention](https://raw.githubusercontent.com/lotuswhl/lotuswhl.github.io/master/img/attention/image-description-attention-sample.png)

# 意图机制与记忆
* 其实从上面的分析来看,实际上意图本身并不像我们真正希望的那样,直接关注特定输入以给出具体的输出.  
* 实际上意图的工作机制更像是从记忆中进行检索,也就是需要预测输出时,我们根据当前已经产生了的输出从之前的输入记忆中去搜索需要关注的信息.然后用于给出输出.

**关于意图机制的使用最新进展,仍待进一步跟进.**
