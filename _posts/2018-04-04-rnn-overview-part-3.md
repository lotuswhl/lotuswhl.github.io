---
layout:     post
title:      循环神经网络概览系列之 - part 3
subtitle:   RNN overview 系列之(RNN基础)
date:       2018-04-04
author:     HL
header-img: img/rnn-lstm.png
catalog: true
tags:
    - deep learning
    - rnn
    - lstm
---

# 介绍
这一章节将会简单的介绍下RNN的初始版本,以及他的历史及各种演化.关于RNN,我在wordpress上写过一篇初探性的文章,建议先阅读它,<a href="https://lotuswhl.wordpress.com/2018/05/12/rnn-%e5%be%aa%e7%8e%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e5%88%9d%e6%8e%a2/" target="_blank">See</a>再简单阅读下面的内容,因为我会跳过其最基础的东西.

# 早期的RNN设计问题
早期的RNN的设计也是有着很多的历史原因的,实际上RNN的设计灵感来源于Hopfiled 网络.具体的我并没有去了解,但是他大概解决了一些模式识别的问题,尤其是对于倒塌的模式中回复模型的问题.后期Boltzmann Machine 和 Auto-encoder也是来自于他. RNN的设计来自于他的改进版,也就是使用固定权重来解决序列问题的模型,RNN则使用了一个自循环的连接,用来解决长期依赖的问题.
> 简单的概括下,RNN的本质就是引入时间特性,当然时间隐层的输入来源于上一时间隐层的输出以及这一时间的新的输入;这一时间的输出来自于这一时间隐层的输出.  

### 一个简单公式足以说明一切
$$h^{(t)}=\delta{(W_{hh}h^{(t-1)}+W_{xh}X^{(t)}+b_h)}$$

# 训练RNN网络的难题
在训练RNN的时候,因为隐层权值共享的问题,对于时间步骤$t$之前的时间步骤$\tau$而言,他们的梯度贡献需要乘上权重W的指数$t-\tau$次,因此存在着梯度的爆炸和弥散的问题.
> 我们很容易的看出如果W的值大于1,那么容易引起梯度爆炸,而如果小于1则容易引起梯度弥散的问题.事实上,对于前面时间步骤的节点的梯度,还要乘上一个因为隐层激活函数$\delta$的梯度引发的问题!起初RNN使用的是tanh激活函数,但是因为tanh函数的导数为$1-tanh^2$,因此会导致梯度弥散的问题;后来relu在前馈神经网络中变得很流行,但是引入relu很明显会导致梯度消失或者弥散或者爆炸的问题,取决于W(因为relu的导数非0即1).  

之后有着各种尝试改良的方法:  
* 梯度正则化训练方法;给梯度加上正则项,迫使梯度不爆炸也不弥散
* TBPPT(截断BPTT): 将梯度进行截断以解决梯度爆炸的问题;但是这样一来便牺牲了长期依赖的能力.
* LSTM(_Coming Soon_)


**未完,待续.**