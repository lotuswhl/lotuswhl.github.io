---
layout:     post
title:      recurrent neural networks an overview
subtitle:   关于RNN的系统探索
date:       2018-04-03
author:     HL
header-img: img/rnn-lstm.png
catalog: true
tags:
    - deep learning
    - rnn
    - lstm
---

# 概要
<p>
深度学习目前已经取得了太多的辉煌成就,不仅仅在计算机视觉领域,自然语言处理领域,在推荐系统,语音识别等领域也有着不菲的贡献.为了更好的理解人工智能,或者为了让计算机更好的为人类服务,目前机器学习和深度学习在图像识别,对象检测等视觉领域,在文本分类,情感分析等自然语言处理领域取得了突破性的进展.
</p> 
<p>
然而我们日常生活中往往需要更多的面对序列数据,比如Image Caption,Speech Synthesis,以及音乐生成(music generation)等等都需要可以输出序列的模型[one to many rnn model].而在其他的一些领域,比如时间序列预测,视频分析和音乐信息检索等,则需要可以接受序列数据作为输入的模型[many to one rnn model].此外,对于机器翻译,机器人控制,可能就需要同时具备可以接受序列数据和输出序列数据的模型[many to many].
</p>
<p>
循环神经网络(RNN)是一类利用循环节点来捕获序列数据的动态特性的连接模型.不像前馈神经网络,RNN理论上可以捕获任意长度上下文窗口的信息. 尽管传统上来说RNN难以训练,并且经常包含上百万的参数,但是由于最新的研究进展,在优化算法,网络结构以及并行计算等方面取得了巨大的进展,使得RNN得以成功的运用于大规模的序列学习任务. 近些年,诸如LSTM(Long Short Term Memory),Bidirectional RNN等网络模型在诸多机器学习任务如机器翻译,Image Caption和手写识别等任务上取得了极大的性能突破.
</p>

# 介绍
<p>
近几年深度学习在有监督学习和无监督学习任务都取得了瞩目成就.这些方法尤其适用于机器感知任务,因为他们底层的特征往往是难以独立解释的,而神经网络模型可以学习到层次化的信息.这不同于传统的方法,他们往往适用于使用手工特征工程的任务.随着现在在存储上取得进展,我们可以拥有更多的数据集,并且基于并行计算对计算机计算能力大幅提升,浅层的线性模型往往倾向于under-fit,因此深度模型可以更好的发挥计算机的计算能力,并可以利用更多的数据资源构建出更好的模型.深度卷积神经网络通过利用视觉信息的局部相关性在诸多重要的应用上取得了记录性的成果.
</p>
<p>
尽管深度神经网络取得了诸多成就,但是他们都基于对样本的独立性假设.也就是他们都是假设输入的样本之间是独立的.(可以很容易看出,对于神经网络的输入,通常每次都是单个样本作为输入,计算输出,与之前的样本或者之后的样本无关.注意:即便是使用batch或者mini-batch进行训练,实际上在模型预测时也是对单个样本做预测).然而这对于视频的帧信息,音乐的判断或者文本语句的word显然是不成立的(独立性假设不成立).此外,对于前馈神经网络模型通常都是接受固定长度的输入向量,输出固定长度的值.因此这对于序列数据或时间数据是很不方便的.而循环神经网路可以利用其包含的隐层状态,理论上可以捕获任意长度的上下文信息,它不需要独立性假设,因此可以很好的应用于时间/序列数据.循环神经网络其输入是以序列元素为单位,也就是一次输入一个序列中的元素,并且由于隐层状态的存在,他可以选择性的通过一些元素,不通过另一些元素,从而很好的解决了输入序列元素之间依赖关系.
</p>

## why model sequentiality explicitly ?
显然支持向量机(SVM),逻辑斯蒂回归(LR)以及前馈神经网络基于独立性假设并且没有捕获时间特征也获得了不错的效果. 可以论证的来说,独立性假设在很多的机器学习任务上取得了不错的进展. 而进一步来说,很多模型也隐式的捕获了时间特性,比如通过将当前的输入与其前后节点拼接到一起作为输入,构建了一个滑动窗口上下文.  **然而尽管如此,即便独立性假设带来了不错的效果,他却抛弃了长期依赖(long term dependency).**

