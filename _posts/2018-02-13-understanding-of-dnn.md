---
layout:     post
title:      关于神经网络的一些思考
subtitle:   thinking changes who you are
date:       2018-01-03
author:     HL
header-img: img/neural-networks.jpg
catalog: true
tags:
    - deep learning
    - thinking
---

## 关于神经网络
通常意义上来说，我们至少可以从神经网络的来源进行思考。NN的构建是来自于生物神经科学的思考，一开始也确实为了模拟大脑神经的思考而构建的用于解决人工智能问题的方案。当然，随着现在深度学习在各大领域的精彩表现，其实其来源或者其根本就没那么重要了。当然，并不是说神经网络本身的原理不重要，那当然很重要，因为我们需要思考神经网络的本质，他为什么可以解决那么多的问题，而是或许我们不需要完全按照生物神经或者人脑的思考方式去了解去思考神经网络。或许，按照很多人的观点，将神经网络作为一个函数近似器更为合适。也就是将神经网络当做一个工具来用，来解决实际问题。

## 神经网络的本质理解与思考
* 如果将神经网络理解为一个函数近似器，其实也是很合适的。神经网络在人工智能的各大领域大显身手。尤其是视觉领域的问题，使用CNN及其各种扩展架构来完成对图像的分类，识别，物体的检测，实体的分割等等问题取得了突破性的进展。对于卷积网络的本质理解已经有很多研究论文给出了相当精彩的分析，一如卷积网络参数，从浅层网络参数对物体基本元素、轮廓等的识别到深层网络参数对物体局部构件的识别，看起来与人眼观察物体的过程很相似，再如对卷积核的期望输入的探索，我们发现不同的卷积核期待的输入的特点，或是窗户，或是板凳或是脸，鼻子耳朵等等，这些都表明卷积网络学习到了很多有价值的信息。
* 然而，除了卷积网络在图像领域的可解释部分之外，目前对于深度神经网络的能力的探索依旧有着很多不可理解的问题。比如将深度神经网络应用于自然语言处理的问题，我们很难理解网络参数的含义，他究竟在捕获什么，是否真如我们所想的那样，能够捕获文本的语意特征。
* 再者，对于机器学习或者人工智能的很多其他子领域，对于深度学习的利用，基本是将其作为一个工具来使用(或者说，用一个我并不想用的词“黑盒子”来使用) 

<p><strong>
结合对网络上各种关于神经网络的思考，我觉得神经网络本身就是一个函数近似器。网络的层级深度大小，每一层节点数量，节点之间连接方式决定了网络的复杂度，决定了网络的capability，同样的也决定了这个函数近似器的表现力。
</strong></p>
<p><strong>
神经网络在学习的过程中，利用反向传播等传递错误的方法来纠正网络的参数，让其达到我们的目的。我们可以认为网络本身在拟合训练数据的本身的分布特征，或者说网络本身在拟合训练数据的输入输出的映射函数！
</strong></p>

## 神经网络的训练过程是在做什么
<p><strong>
我们可以想象，神经网络作为一个函数近似器，其参数空间表达着其函数空间。而我们的训练过程也就是在这个函数空间中找到最优的那个函数！(当然也就是他的最优参数)。怎么样才叫最优参数呢，当然，也就是将训练数据的输入可以完美或者尽可能完美的映射到其正确的输出。我们可以想象，从网络的输入，到网络的输出之间有着“无数”的通道，而训练的目的在于，对于那些走错了方向的通道，我们慢慢的将它拉回或者推回正道！一旦所有的管道都找对了方向，我们的目标也就达到了：训练数据的输入就可以沿着这些管道走向正确输出方向！
</strong></p>

