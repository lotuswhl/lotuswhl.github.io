---
layout:     post
title:      生成对抗网络研究系列之-part3
subtitle:   GAN是如何工作的
date:       2018-05-23
author:     HL
header-img: img/gan.jpg
catalog: true
tags:
    - deep learning
    - gan
---

# 生成对抗网络框架
GAN由生成模型和判别模型构成，简单的说它是一个两个玩家的游戏。其中一个称之为生成器，他的目标是欺诈判别器生成假的样本数据来以假乱真，而另外一个称之为判别器，它的目的是尽全力区分真假样本，也就是区分样本是来自于真实数据集还是由生成器生成。这里的判别器是一个传统的有监督分类器，而判别器则尝试去欺骗判别器。**一个很经典的比喻：我们可以认为判别器是银行造币者，而生成器为制造假币的人，那么判别器则会通过比较分析真实钱币与假币之间的差距，进而区分出真假币;而造假币的人则可以通过某种渠道获知银行造币者的判别结果，进而利用这个判别结果来升级自己的技术，以进一步欺骗银行造币者，形成一个两玩家相互博弈的的游戏。**  
****
判别器和生成器可以由两个函数表示，这里通常来说就是我们的目标函数。这两个函数都分别对他们的输入和参数可导。生成器G对他的输入$z$以及参数$\theta^G$可导，判别器则对他的输入x,和参数$\theta^D$
可导。这两个玩家都分别基于自己的参数定义了自己的损失函数。生成器的目标是训练参数$\theta^G$从而最小化目标损失函数$J^G(\theta^G,\theta^D)$。判别器的目标是最小化损失函数$J^D(\theta^D,\theta^G)$。虽然每个函数都包含了彼此的参数，但是他们却智能控制自己的参数。训练或者优化的目标是达到一个最小值或者局部最小值，也就是在当前参数的周围取值都将导致更大损失值。最后，将会达到一个平衡态，也就是损失函数$J^D$对$\theta^D$最小，损失函数$J^G$对$\theta^G$最小。

## 生成器
生成器是一个可微的函数，通常我们会使用一个神经网络来表示生成模型。生成器接受来自于一个先验分布的向量z（也可以解释为从潜变量空间取样）为输入，然后通过生成模型得到生成样本x（该样本x则来自于生成模型分布$p_{model}$ ）。事实上生成模型并非只能从第一层也就是输入层接受输入，而在生成模型的任意一层都可以接受输入，比如可以取两个向量$z_1,z_2$分别输入到第一层和最后一层。更为常见的可能是在中间层加入噪声，或者拼接噪声向量。**另外一个值得关注的问题是：如果我们希望生成模型可以完全支持目标数据空间，那么z的向量空间应该至少与x的空间一样大。**

## 训练过程
训练的过程通常是使用基于SGD的方法，也就是取一个mini-batch的x数据于真实数据，再从模型的先验分布中去一个mini-batch的z数据，然后同时更新两组参数：更新$\theta^G$以减少$J^G$,更新$\theta^D$以减少$J^D$。通常最简单的方式就是两组参数各更新一次：先以取样的x值计算D的损失，然后再以z值通过G生成的fake样本再计算D的损失，然后根据这两个的损失和更新参数$\theta^D$，接着根据z生成的fake样本来更新$\theta^G$以降低$J^G$。
# 损失函数

## 判别器的损失函数

## minimax game

## 启发式，非饱和游戏

## 最大似然游戏

## GAN的divergence的选择是不是它的显著特质

## 损失函数的比较

# DCGAN的结构

# GAN与NCE（Noise-Contrastive Estimation）以及MLE（Maximum Likelihood）的关系

