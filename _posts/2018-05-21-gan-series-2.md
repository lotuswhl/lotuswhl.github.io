---
layout:     post
title:      生成对抗网络研究系列之-part2
subtitle:   生成模型的工作原理,GAN与其他生成模型的比较
date:       2018-05-21
author:     HL
header-img: img/gan.jpg
catalog: true
tags:
    - deep learning
    - gan
---

# 简介
到目前来说,我们知道了生模型的重要性以及他们可以做什么,接下来就需要思考生成模型是怎么工作的?生成对抗网络GAN与其他的生成模型有何不同?
# 从极大似然说起
* **极大似然是一种度量modle estimator好坏的准则。**  
* **极大似然的一种直观的理解就是：假说我们定义了一个模型的用于估计输入数据(样本)的分布，那么模型的参数 $\theta$ 也就定义了一族函数族$P_{modle}(X;\theta)$ ;极大似然的目标就是求使得$P_{modle}(X;\theta)$最大的参数$\theta$值。**  
$$\theta _{ML}=argmax_\theta P_{model}(X;\bf \theta)$$
$$=argmax_\theta \prod_{i=1}^{m}P_{model}(X^{(i)};\bf \theta)$$
在求解极大似然时，为了方便数值计算，防止数值下溢(多个小概率的乘积，并且为了后续求解最优参数值$\theta$的方便，通常会对极大似然取对数似然。因为对数函数的单调递增的，并且取对数对极大似然的最优解不会有变化。So:  
$$\theta _{ML}=argmax _\theta \sum_{i=1}^m\log P_{model}(X^{(i)};\bf \theta)$$    
**以上:$X^{(i)}$表示的是第i个样本向量值，假设有m个样本。**    
因为实际上我们只能对训练数据求取极大似然值，所以上面的公式可以改为:  
$$\theta _{ML}=argmax _\theta E_{x\sim \hat{P}_{data}}\log{P_{model}(X^{(i)};\bf \theta)}$$  
也就是根据样本数据，求取极大似然值的期望。注意其中的$\hat P_{data}$表示的是样本数据的分布，用以近似真实数据的分布。
### 另外一种解释极大似然的方法KL:散度
事实上，在求取极大似然的过程中我们发现我们的目标在于使得模型的概率分布更接近数据的真实分布，因为我们无法知晓数据的真实分布，因此代之为训练数据的分布;而度量两个概率分布之间的距离的一常用的方法就是KL Divergence。其计算公式如下:  
$$KL(\hat P_{data}||P_{model})=E_{x \sim \hat P_{data}}(\log {\hat P_{data}(x)} - \log {P_{model}(x)}  )$$  
$$=\sum_{i=1}^m \log{\hat P_{data}(X^{(i)}) \frac{\log {P_{model}(X^{(i)})}}{\log{\hat P_{data}(X^{(i)})}}}$$  
  
* 我们更常见的是后面将期望展开后的形式。Anyway。

**根据上面的公式，我们会发现上面等对KL散度的最小化可以转换为:
$-E_{x \sim \hat P_{data}} [\log{P_{model}(x)}]$ ;**
因为，对$\log \hat P_{data}$的期望实际上在数据生成的过程中就已知了，也就是无需也无法优化，我们只需要优化后面的部分，从而使得，模型的分布更佳接近于数据的分布。
#### 关于熵、交叉熵、KL的小插曲
有兴趣的可以参见之前在[CSDN博客](https://blog.csdn.net/dragonboss2016/article/details/80186078)上写过的对这几个概念的一点点理解。

## 谈谈极大似然在条件概率分布上应用以及MSE
在大多数机器学习的任务中，我们可能更为关心的是条件概率，也就是对于大多数有监督学习而言$P(Y|X;\theta)$才是我们所关心的。也就是将输入数据X，映射到目标(标签)Y上。
> **这也就表明极大似然本身的目标在于拟合概率分布;因此，对于不同类型的概率分布都应适用。**