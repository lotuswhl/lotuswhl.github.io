---
layout:     post
title:      生成对抗网络研究系列之-part2
subtitle:   生成模型的工作原理,GAN与其他生成模型的比较
date:       2018-05-21
author:     HL
header-img: img/gan.jpg
catalog: true
tags:
    - deep learning
    - gan
---

# 简介
到目前来说,我们知道了生模型的重要性以及他们可以做什么,接下来就需要思考生成模型是怎么工作的?生成对抗网络GAN与其他的生成模型有何不同?
# 从极大似然说起
* **极大似然是一种度量model estimator好坏的准则。**  
* **极大似然的一种直观的理解就是：假说我们定义了一个模型的用于估计输入数据(样本)的分布，那么模型的参数 $\theta$ 也就定义了一族函数族$P_{modle}(X;\theta)$ ;极大似然的目标就是求使得$P_{modle}(X;\theta)$最大的参数$\theta$值。** 


$$\theta _{ML}=argmax_\theta P_{model}(X;\bf \theta)$$  

$$=argmax_\theta \prod_{i=1}^{m}P_{model}(X^{(i)};\bf \theta)$$ 

在求解极大似然时，为了方便数值计算，防止数值下溢(多个小概率的乘积，并且为了后续求解最优参数值$\theta$的方便，通常会对极大似然取对数似然。因为对数函数的单调递增的，并且取对数对极大似然的最优解不会有变化。So:

$$\theta _{ML}=argmax _\theta \sum_{i=1}^m\log P_{model}(X^{(i)};\bf \theta)$$

**以上:$X^{(i)}$表示的是第i个样本向量值，假设有m个样本。** 

因为实际上我们只能对训练数据求取极大似然值，所以上面的公式可以改为:

$$\theta _{ML}=argmax _\theta E_{x\sim \hat{P}_{data}}\log{P_{model}(X^{(i)};\bf \theta)}$$ 

也就是根据样本数据，求取极大似然值的期望。注意其中的$\hat P_{data}$表示的是样本数据的分布，用以近似真实数据的分布。
### 另外一种解释极大似然的方法KL:散度
事实上，在求取极大似然的过程中我们发现我们的目标在于使得模型的概率分布更接近数据的真实分布，因为我们无法知晓数据的真实分布，因此代之为训练数据的分布;而度量两个概率分布之间的距离的一常用的方法就是KL Divergence。其计算公式如下:

$$KL(\hat P_{data}||P_{model})=E_{x \sim \hat P_{data}}(\log {\hat P_{data}(x)} - \log {P_{model}(x)}  )$$   

$$=\sum_{i=1}^m \log{\hat P_{data}(X^{(i)}) \frac{\log {P_{model}(X^{(i)})}}{\log{\hat P_{data}(X^{(i)})}}}$$  
  
> 我们更常见的是后面将期望展开后的形式。Anyway。

**根据上面的公式，我们会发现上面等对KL散度的最小化可以转换为:
$-E_{x \sim \hat P_{data}} [\log{P_{model}(x)}]$ ;**
因为，对$\log \hat P_{data}$的期望实际上在数据生成的过程中就已知了，也就是无需也无法优化，我们只需要优化后面的部分，从而使得，模型的分布更佳接近于数据的分布。
#### 关于熵、交叉熵、KL的小插曲
有兴趣的可以参见之前在[CSDN博客](https://blog.csdn.net/dragonboss2016/article/details/80186078?target="_blank")上写过的对这几个概念的一点点理解。

## 谈谈极大似然在条件概率分布上应用以及MSE
在大多数机器学习的任务中，我们可能更为关心的是条件概率，也就是对于大多数有监督学习而言$P(Y|X;\theta)$才是我们所关心的。也就是将输入数据X，映射到目标(标签)Y上。
> **这也就表明极大似然本身的目标在于拟合概率分布;因此，对于不同类型的概率分布都应适用。**

X表示我们的输入，Y表示我们的目标，那么条件最大似然可以表示为:  

$$\theta_{CML}=argmax_\theta{P(Y|X; \theta)}$$  

假设我们的样本是i.i.d的，那么可以写为:  

$$\theta_{CML}=argmax _\theta  {\sum_{i=1}^m \log P(y^{(i)}|X^{(i)};\theta)}$$  

### 举个例子:线性回归
假设我们的样本中，对于某个输入x，其对应的输出y可能有多个，我们假设对于任意一个样本x，其输出值服从高斯分布;那么我们可以得到以下条件似然:  

$$P(y|x;\theta)=\mathcal{N}(y|\hat y(\bf{x;w});\delta ^2)$$  

接下来可以按照m个独立同分布样本假设得到对数似然:  

$$\sum _{i=1}^m \log P(y^{(i)}|X^{(i)};\theta)$$  

$$=-m\log \delta -\frac{m}{2}\log(2\pi) - \sum _{i=1}^m {\frac{(y-\hat y)^2}{2\delta ^2}}$$  

在这里$\hat y^{(i)}$ 是对应于样本输入$X^{(i)}$ 的输出。

我们再看，如果使用MSE error来进行度量:  

$$MSE_{train}=\frac{1}{m} \sum _{i=1}^m (y-\hat y)^2$$  

**我们发现:极大似然的前边两项是常量，其最右边的一项等价于MSE，我们可以看出最大化似然值等价于最小化MSE error！因此，在这种情况下，也就是model的参数空间分布假设为高斯分布的情况下，使用极大似然estimator等价于MSE 估计。反之，这也恰好佐证了，使用MSE error 可以作为极大似然的估计程序！(判别方法！)**

### 小插曲:极大似然的作为生成模型的本质
* 首先，简单理解，极大似然的目标是从模型的函数空间中找到最优的参数也就是最优的函数，用于近似数据分布。
* **条件似然分布的特点是除了有着模型参数空间外，引入输入x作为先验条件。也就是model需要构建基于X的预测目标$\hat y$ 分布以近似数据基于X的目标$y$ 分布。**

那么极大似然作为生成模型，如何度量模型输出的好坏呢?我们通常可以使用最简单的MSE度量，也就是计算预测目标与真实目标之间的距离。当然也可以选择其他的不同的度量方法，我们上面已经证明了MSE error度量和极大似然的目标等价性。**事实上，对于生成模型而言，判别输出目标与真实目标之间的距离有着很多的方法，甚至可以使用一个判别网络来进行度量！这就是GAN生成对抗网络的由来！(别急！后面会继续讲！)因为好的度量方式，可以让生成模型可以更好的学习输入数据的真实分布！**

## 极大似然的几点属性
极大似然的一致性属性理解:
* 模型分布的存在性。首先，$P_{data}$ 必须要落在模型的假设空间$P_{model}(.;\theta)$中。这个不难理解，如果不这样的话，那么即便是找到了所谓的最优解$\theta_{optimal}$ 也无法完全正确的恢复真实数据分布$P_{data}$.
* 模型参数的存在唯一性。真实数据分布$P_{data}$ 必须对应于且只对应于模型参数的唯一一个$\theta$ . 只有这样，极大似然才可以确定原始数据的生成所使用的参数。否则，如果满足上面的条件，那么意味着模型有多个参数值可以生成或者说复原输入数据，只是无法确定真实对应的参数值。

# 生成模型的分类
前面曾说过，即便并非所有的生成模型都是使用的极大似然准则，但是大多数模型都存在着极大似然的变种，因此为了方便比较各种模型，我们将他们统一使用极大似然principle以进行对比。  

![taxonomy of genrative models](https://raw.githubusercontent.com/lotuswhl/lotuswhl.github.io/master/img/gan/taxonomy-of-generative-models.png)  
对于上面这张图，每一个叶子结点都对应着具体的生成模型。其他的很多叶子结点都有着这样那样的缺点，GAN的设计就是为了解决这些缺点，但同时GAN本身却也带来了新的缺点。

# 显示密度模型(Explicit Density Models)

