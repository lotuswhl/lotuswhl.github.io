---
layout:     post
title:      生成对抗网络研究系列之-part4
subtitle:   GAN的训练技巧Tips&Tricks
date:       2018-05-25
author:     HL
header-img: img/gan.jpg
catalog: true
tags:
    - deep learning
    - gan
---

# 简介
生成对抗网络的训练技巧还是比较复杂的，为了提升GAN的performance，在实践中会引入很多的技巧来进行训练。之所以说其训练复杂，还有另外一个原因，也就是对于不同的问题，不同的训练技巧未必是用，很多时候我们也并不清楚究竟为什么这些技巧适用，为什么不适用。这篇POST将会对一些较为重要的应用较为广泛的技巧进行简单的说明。后续或许会在这里进行更新，也可能会在我的另外一个博客网站上更新。
## 使用标签数据训练
**当你拥有标签数据时，尽量使用带标签数据训练判别模型。**  
****
实践证明，使用Conditional-GAN可以使得GAN生成指定类型的样本，而引入标签训练判别模型同样可以提升生成样本的质量。
#### 简单的原因分析
那么为什么引入标签数据训练判别模型可以提升生成器生成样本的质量呢？一般而言有两种说法：其一认为引入标签数据也就是引入类别信息可以提供给训练过程有价值的信息进而帮助优化训练。其二认为生成的样本本身并没有变化，而有标签的判别模型使得生成器得以更关注用户人的视觉系统中更关注的信息。如果是后者的话，那么这将会违背GAN的本意，因为生成器将无法学习到数据的真实分布，但是将会帮助构建一些供人赏玩的媒体系统，比如说可以帮助RL的Agent参考环境当中与人更相关的方面。
## 单侧标签平滑（one-sided label smoothing）
**在训练时可以平滑正例标签概率从1到0.9或者增加随机性：在0.8~1.2之间随机取值，而对于fake样本不平滑。**  
****
从原理上分析降低正例样本的目标概率值，可以使得判别器降低那些很自信的输入样本的logits值，将他们缩减到一个较小一点的值。
#### 正则化分析
标签平滑的方法在早起就有使用，将其应用于对象检测的正则化方法。使用标签平滑作为正则化方法表现很好的一个很重要的原因在于标签平滑方法并不鼓励或者促使模型选择错误的分类数据，而对于其他的正则化方法比如weight-decay则可能会因为压制模型的权重而错误的鼓励了误分类。  
**进一步思考标签平滑为何可以作为正则化方法：可以想象，如果将正例的目标值设为1.0，那么模型会尽最大努力学习样本正例的特点进而与反例或者误分类样例进行区分，也就是模型将会学习到训练数据中特定类别的特定的信息，而可能会因此而学习到一些噪声，进而导致过拟合问题。而标签平滑可以帮助模型在训练时进行泛化思考，亦即考虑使用那0.9或者90%的泛化特征而放弃那些虽然提高了预测概率的那剩下的0.1的特征（这部分特征信息很可能是噪声信息）。**

## 虚拟batch normalization
主要是针对mini-batch样本的特征的均值和方差的不均衡或者不稳定问题；可有通过引入参考或者引用batch sample来 进行平衡；通过引入refference batch来帮助平衡样本的均值和标准差，但这样很明显会带来一个问题，也就是对refference batch过拟合。因此可以引入一个虚拟batch的概念，也就是在真实训练样本batch和refference batch之间取一个union来进行平衡，从而使得mini-batch的归一化过程在样本之间是独立的，那么对于生成器生成的样本也是独立同分布的（**可以思考，如果生成器将输入的噪声分布进行normalize，那么显然也就约束了一个mini-batch的样本之间的关联性，这也是为什么DCGAN将生成器的最后一层不加batch-norm，目的就是为了让生成器可以学习到样本真正的均值和误差等统计特性。**）。

## D和G的平衡问题
G和D是否需要平衡呢？简单的回答：不完全需要，虽然我们一直在追求D和G之间的一个平衡态，但是理论和实践证明让判别器稍胜一筹也可以获得不错的结果。当然，这里需要进行简单的说明：因为判别器在学习模型的概率和样本真实数据的概率的比值，因此我们可以让判别器稍微高一点，这样并不影响整体的结果，甚至是好消息。**一个更为重要的原因：生成器有着mode倒塌的问题，也就是生成器倾向于学习比真实数据的更少的mode，这里与GAN的目标损失的reverse-KL有关。因此，我们需要判别器学习更多的模式，从而进一步让生成器可以有更多的选择。**  


以上。